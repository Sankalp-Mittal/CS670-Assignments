## To be completed

# CS670 – A1 (MPC with Du–Atallah) — README

> **Heads-up (correctness disclaimer)**
> The current implementation **does not reconstruct the correct answer** in general. There are still bugs in the vector update/communication path that I couldn’t fully resolve before submission. I’ve left extensive debug prints, wire-format notes, and a checker to help you see where it goes wrong.

---

## What this project does

Three containers simulate the parties:

* **gen** – deterministically **generates data**: secret shares of the user/item matrices and the per-party query files.
* **p2** – a dealer/server that **streams correlated randomness** (Du–Atallah shares) to both clients.
* **p0** and **p1** – the two clients. They receive the server’s shares, sync with each other, read their local query file, exchange masked vectors, and then try to perform a **secure vector scaling / update** per query.

All files are written to a bind-mounted host directory so you can inspect results after the containers stop.

---

## How to run (Docker & Compose)

### Prerequisites

* Docker Desktop (or Docker Engine) and Docker Compose v2
* Linux or Windows/macOS with WSL2 enabled for Docker Desktop

### Build & run

From the repository root:

```bash
docker-compose build
docker-compose up
```

This will:

1. **Build** a single image (`mpc:latest`) that contains all three binaries: `gen_queries`, `p2`, and the client binary used for both `p0` and `p1`.
2. **Start services** in order (per `docker-compose.yml`):

   * `gen` runs **once**, writes input files under `./mpc_data`, then **exits 0**.
   * `p2` starts and **listens on port 9002** for the clients. It streams `q` pairs of Du–Atallah shares (and multiplication triples, if enabled by the code).
   * `p0` and `p1` start, **connect to p2**, read their **query files** from `/data`, then try to execute the protocol in lockstep over a peer connection (`p0↔p1` on port 9001).

You’ll see logs from all services multiplexed in your terminal. To stop, hit `Ctrl+C`.

### What the `docker-compose.yml` does (execution instructions)

* **Single image, multiple roles**
  All services use the same built image but run different binaries/entrypoints baked into the image.
* **Volumes**

  ```yaml
  volumes:
    - ./mpc_data:/data
  ```

  Everything the programs read/write is under `/data` in the container, which maps to `./mpc_data` on your host. This makes artifacts persistent.
* **Dependency ordering**

  ```yaml
  depends_on:
    gen:
      condition: service_completed_successfully
  ```

  `p2` waits for `gen` to finish successfully (so input files exist). `p0` and `p1` depend on `p2` (so the dealer is listening before clients connect).
* **Network**
  A user-defined bridge network `mpc_net` allows service name resolution: the programs connect to hosts `p2` (dealer) and `p1`/`p0` (peer) by **service name**.
* **Parameters**
  The `gen` service’s `command` controls the run:

  ```yaml
  command: ["<m>", "<n>", "<k>", "<q>", "--seed=42", "--debug"]
  ```

  Change these to generate different sizes (users m, items n, features k, queries q).

---

## Files you’ll see in `./mpc_data`

Generated by **gen**:

* `/data/p0_shares/p0_U.txt` – P0’s share of the user matrix $U$
* `/data/p1_shares/p1_U.txt` – P1’s share of $U$
* `/data/p0_shares/p0_queries.txt` – P0’s query file (each line: `user_idx` followed by a length-`k` item vector share for P0)
* `/data/p1_shares/p1_queries.txt` – P1’s query file (same structure for P1)
* `/data/plain_UV.txt` – **debug only**: true $U$, $V$, and queries (not given to parties in a real protocol)

Written by **clients** (for debugging):

* `/data/client0.shares`, `/data/client1.shares` – the Du–Atallah share tuples $X, Y, z$ streamed by `p2`
* `/data/client0.results`, `/data/client1.results` – per-query “updated share” rows the client attempted to compute

---

## How it’s supposed to work (high level)

* The dealer (`p2`) generates, per query, a Du–Atallah tuple $(X, Y, z)$ of length $k$.
* Each client holds an **additive share** of a user vector $u$ and constructs a query that contains $\text{user\_idx}$ and a **local item share** $v$ of length $k$.
* For a given query, both clients exchange masked vectors:

  * P0 sends $X + u_0$ and $Y + v_0$; P1 sends $X + u_1$ and $Y + v_1$.
* Each computes a **local scalar**

  $$
  \Delta_i = u_i \cdot (v + Y_{\text{peer}}) \;-\; v_i \cdot (X_{\text{peer}}) \;+\; z_i
  $$

  so that $\Delta_0 + \Delta_1 = 1 - (u\cdot v)$ (this folds the protocol’s $\alpha$ as a **scalar**).
* Each scales the **item** vector share by the same scalar factor $(1 - \Delta)$ using 2PC multiplication per coordinate (Du–Atallah triples), then updates the user share:

  $$
  u_i \leftarrow u_i + \big((1-\Delta)\cdot v\big)_i
  $$
* Finally, the updated user row is written back for debugging.

> **Note:** The above is the intended flow. The current code follows this shape but has bugs (see below), so the reconstructed value is not reliable.

---

## Known issues / why the answer isn’t correct

* **Vector build bug was fixed**, but there may still be places where `reserve()` is used without `resize()` before indexed writes. That leads to empty payloads or garbage.
* **Delta formula was wrong earlier** (`s.Y` used instead of `item_share` in one term). I patched it, but subtle mistakes can persist across paths.
* **Wire format (endianness/signedness)**
  We send 64-bit integers in big-endian as `uint64_t` on the wire and convert to/from `long long` in memory. Any mismatch of sizes or casts can silently corrupt values for very large magnitudes.
* **Lockstep/barriers**
  The P0↔P1 barrier is minimal. If either side misses a step, both coroutines can stall or proceed with mismatched indices.
* **Multiplication (vector scaling)**
  The Du–Atallah scalar multiplication per coordinate is implemented, but if the indexing into the triple pool $(q \times k)$ is off by one or misaligned across parties, the product is wrong while the program still “runs”.

Because of the above, reconstructed rows do not match ground truth updates in the checker for certain runs.

---

## Checker (optional)

There’s a simple `checker.py` that:

* Reads `plain_UV.txt`
* For each query `(i, j)` (or `(user_idx, item_share)` in the modified format), computes
  $\delta = 1 - U_i \cdot V_j$ and then
  $U_i \leftarrow U_i + \delta \cdot V_j$
* Compares the sum of client results vs. ground truth and prints the max absolute diff.

> The checker is only for debugging the algorithmic intent; it doesn’t validate communication.

---

## Changing problem size

Edit `docker-compose.yml` under **gen**:

```yaml
command: ["<m>", "<n>", "<k>", "<q>", "--seed=42", "--debug"]
```

Example:

```yaml
command: ["10", "10", "5", "3", "--seed=42", "--debug"]
```

Then re-run:

```bash
docker-compose up --build
```

---

## Clean up

* Stop all: `Ctrl+C`
* Remove containers: `docker-compose down`
* Remove artifacts: `rm -rf mpc_data` (careful — this deletes generated files)

---

## Communication rounds & efficiency (brief)

* **Dealer → Clients:** For each query, 2 lines of length-$k$ vectors (X and Y) + one scalar $z$ per client. One final `"OK"` sentinel ends the stream.
  *Asymptotic per query per client:* $O(k)$ integers.
* **P0 ↔ P1 (per query):**

  * One **barrier** (2 small messages total).
  * One **vector exchange**: two length-$k$ vectors each way (masked $X+u$, $Y+v$).
  * For scaling with triples: **two scalars per coordinate** (send+recv) → $O(k)$ integers each direction.
* **Total rounds per query:** a small constant (barrier + vector swap + per-coordinate triple exchange).
  The current code prioritizes clarity over throughput; batching or packing would reduce syscalls.

---

## Final word

The code prints the shares it receives and the per-query “updated” rows for debugging, and it’s fully containerized with persistent output. However, **the final computed result is not correct** due to bugs I couldn’t finish fixing in time. I’ve highlighted the likely culprits and kept the code structured so they’re easy to continue investigating.
